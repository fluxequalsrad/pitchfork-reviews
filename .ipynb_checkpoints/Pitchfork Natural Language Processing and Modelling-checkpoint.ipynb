{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20eaaf8d",
   "metadata": {},
   "source": [
    "# Pitchfork Music Reviews - Natural Language Processing\n",
    "\n",
    "I plan to analyse album reviews published on https://pitchfork.com using natural language processing (NLP) techniques to explore how descriptive language varies across music genres. My aim is to identify whether reviews adhere to a consistent terminology, or if certain music styles have terms and descriptors unique to them. Based on these findings, I will train a classification model to predict the genre of unseen reviews.\n",
    "\n",
    "This notebook covers the application of natural language processing techniques to our review dataset, looks at the how language changes between genres and covers the training and testing of a classification model in applying these findings to unseen reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da78e3",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b020c6",
   "metadata": {},
   "source": [
    "### Text pre-processing\n",
    "- lower casing\n",
    "- tokenisation\n",
    "- remove punctuation\n",
    "- stop words\n",
    "- lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689facba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb87edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned data\n",
    "df = pd.read_csv(\"review_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4dd873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/simoncrouch/Desktop/review_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e24e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all review text to lowercase\n",
    "df['Text'] = df['Text'].str.lower()\n",
    "\n",
    "# split text into individual tokens\n",
    "df['tokens'] = df['Text'].apply(word_tokenize)\n",
    "# drop Text column\n",
    "#df = df.drop(['Text'], axis = 1)\n",
    "\n",
    "# remove punctuation\n",
    "df['tokens'] = df['tokens'].apply(\n",
    "    lambda tokens: [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens])\n",
    "\n",
    "# remove stop words - editing ntlk's list\n",
    "stop_words = set(stopwords.words('english')) - {'no', 'not'}\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: [w for w in tokens if not w in stop_words])\n",
    "\n",
    "# lemmatise text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f52e1c1",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "Aim to identify the most important words for each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6a97c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text \n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "478041c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Lemmatizing and Tokenizing function\n",
    "class WordLemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl=WordNetLemmatizer()\n",
    "    def __call__(self,doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "# removing common non-genre specific music terminology\n",
    "stop_words= stopwords.words('english') + ['like', 'album','music','sound','song','track','record','artist','new','one']\n",
    "\n",
    "# Define vectorizer object\n",
    "vectorizer=TfidfVectorizer(analyzer='word',\n",
    "                           input='content',\n",
    "                           lowercase=True,\n",
    "                           #stop_words= set(stopwords.words('english'))\n",
    "                           # removing common non-genre specific music terminology\n",
    "                           stop_words= stop_words,\n",
    "                           min_df=3,\n",
    "                           ngram_range=(1,2),\n",
    "                           tokenizer=WordLemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f01490b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning:\n",
      "\n",
      "Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create list to store important words\n",
    "important_words_by_genre = {}\n",
    "# define number of words per genre\n",
    "top_n = 40\n",
    "\n",
    "# fit vectorizer on each genre's text data seperately\n",
    "for genre in df['Genre'].unique():\n",
    "    # Filter the text by genre\n",
    "    genre_text = df[df['Genre'] == genre]['Text']\n",
    "    # Fit and transform the vectorizer on this subset\n",
    "    X_genre_tfidf = vectorizer.fit_transform(genre_text)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    # Create a DataFrame to hold the TF-IDF scores\n",
    "    genre_tfidf_df = pd.DataFrame(X_genre_tfidf.toarray(), columns=feature_names)\n",
    "    # Sum TF-IDF scores across all documents within the genre\n",
    "    genre_word_scores = genre_tfidf_df.sum(axis=0).sort_values(ascending=False)[:top_n]\n",
    "    # Store as a DataFrame with words and scores for this genre\n",
    "    important_words_by_genre[genre] = pd.DataFrame({\n",
    "        'word': genre_word_scores.index,\n",
    "        'tfidf_score': genre_word_scores.values\n",
    "    })\n",
    "    \n",
    "# important_words_by_genre['Genre Name'] to access values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7ca77ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>90</th>\n",
       "      <th>across</th>\n",
       "      <th>aesthetic</th>\n",
       "      <th>affiliate</th>\n",
       "      <th>along</th>\n",
       "      <th>already</th>\n",
       "      <th>also</th>\n",
       "      <th>always</th>\n",
       "      <th>another</th>\n",
       "      <th>around</th>\n",
       "      <th>...</th>\n",
       "      <th>wave</th>\n",
       "      <th>way</th>\n",
       "      <th>whose</th>\n",
       "      <th>within</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.105146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117144</td>\n",
       "      <td>0.073541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079874</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072116</td>\n",
       "      <td>0.103108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071764</td>\n",
       "      <td>0.193138</td>\n",
       "      <td>0.086678</td>\n",
       "      <td>0.193138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065845</td>\n",
       "      <td>0.143528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080559</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082894</td>\n",
       "      <td>0.090345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091040</td>\n",
       "      <td>0.091040</td>\n",
       "      <td>0.182079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.214514</td>\n",
       "      <td>0.179244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.075018</td>\n",
       "      <td>0.107257</td>\n",
       "      <td>0.053629</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.119496</td>\n",
       "      <td>0.048629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.062842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062842</td>\n",
       "      <td>0.125684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056983</td>\n",
       "      <td>0.095476</td>\n",
       "      <td>0.052029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156577</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.055064</td>\n",
       "      <td>0.061347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061347</td>\n",
       "      <td>0.038513</td>\n",
       "      <td>0.055064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049930</td>\n",
       "      <td>0.083659</td>\n",
       "      <td>0.045589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          90    across  aesthetic  affiliate     along   already      also  \\\n",
       "0   0.105146  0.000000   0.117144   0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.114873   0.000000   0.000000  0.000000  0.114873  0.000000   \n",
       "2   0.086678  0.000000   0.000000   0.000000  0.000000  0.000000  0.071764   \n",
       "3   0.000000  0.000000   0.118148   0.000000  0.000000  0.000000  0.087800   \n",
       "4   0.000000  0.000000   0.000000   0.000000  0.000000  0.000000  0.090345   \n",
       "5   0.000000  0.000000   0.000000   0.091040  0.091040  0.182079  0.000000   \n",
       "6   0.000000  0.059748   0.000000   0.059748  0.000000  0.059748  0.000000   \n",
       "7   0.000000  0.000000   0.110642   0.000000  0.000000  0.000000  0.082222   \n",
       "8   0.062842  0.000000   0.000000   0.070013  0.000000  0.000000  0.156088   \n",
       "9   0.000000  0.000000   0.000000   0.000000  0.085618  0.000000  0.000000   \n",
       "10  0.000000  0.000000   0.000000   0.000000  0.114818  0.000000  0.000000   \n",
       "11  0.055064  0.061347   0.000000   0.000000  0.000000  0.000000  0.045589   \n",
       "\n",
       "      always   another    around  ...      wave       way     whose    within  \\\n",
       "0   0.000000  0.000000  0.117144  ...  0.117144  0.073541  0.000000  0.105146   \n",
       "1   0.000000  0.000000  0.000000  ...  0.000000  0.072116  0.103108  0.000000   \n",
       "2   0.193138  0.086678  0.193138  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.106047   \n",
       "4   0.000000  0.109121  0.000000  ...  0.000000  0.076321  0.000000  0.000000   \n",
       "5   0.000000  0.326860  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.059748  0.214514  0.179244  ...  0.059748  0.075018  0.107257  0.053629   \n",
       "7   0.000000  0.000000  0.000000  ...  0.000000  0.138918  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.062842  0.125684   \n",
       "9   0.085618  0.000000  0.000000  ...  0.000000  0.053749  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  ...  0.000000  0.072081  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  ...  0.061347  0.038513  0.055064  0.000000   \n",
       "\n",
       "        word      work     world     would      year       yet  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.079874  0.000000  \n",
       "1   0.000000  0.000000  0.114873  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.000000  0.065845  0.143528  \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.080559  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.082894  0.090345  \n",
       "5   0.000000  0.000000  0.000000  0.074096  0.000000  0.135310  \n",
       "6   0.059748  0.059748  0.119496  0.048629  0.000000  0.000000  \n",
       "7   0.110642  0.000000  0.221284  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.070013  0.000000  0.056983  0.095476  0.052029  \n",
       "9   0.000000  0.171235  0.000000  0.069684  0.000000  0.063626  \n",
       "10  0.229637  0.000000  0.000000  0.000000  0.156577  0.000000  \n",
       "11  0.000000  0.000000  0.000000  0.049930  0.083659  0.045589  \n",
       "\n",
       "[12 rows x 180 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d4effeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out words less than three letters to remove lemmatizing mistakes\n",
    "electronic = important_words_by_genre['Electronic']\n",
    "electronic = electronic[electronic['word'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "pop = important_words_by_genre['Pop']\n",
    "pop = pop[pop['word'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "rock = important_words_by_genre['Rock']\n",
    "rock = rock[rock['word'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "experimental = important_words_by_genre['Experimental']\n",
    "experimental = experimental[experimental['word'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "rap = important_words_by_genre['Rap']\n",
    "rap = rap[rap['word'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "folk = important_words_by_genre['Folk']\n",
    "folk = folk[folk['word'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "jazz = important_words_by_genre['Jazz']\n",
    "jazz = jazz[jazz['word'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "metal = important_words_by_genre['Metal']\n",
    "metal = metal[metal['word'].apply(lambda x: len(x) > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f0c348f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Electronic",
         "type": "scatter",
         "x": [
          "feel",
          "producer",
          "vocal",
          "drum",
          "club",
          "pop",
          "time",
          "beat",
          "first",
          "even",
          "techno",
          "dance",
          "guitar",
          "ambient",
          "voice",
          "year",
          "house",
          "electronic",
          "band",
          "debut",
          "might",
          "make",
          "work",
          "synths",
          "synth",
          "long",
          "take",
          "world",
          "rhythm",
          "much",
          "space",
          "back",
          "set",
          "yet",
          "melody",
          "across"
         ],
         "xaxis": "x",
         "y": [
          3.818589796209124,
          3.7023832839476034,
          3.4953721590324087,
          3.423403316959108,
          3.3832663181394724,
          3.308755107681813,
          3.261842193258468,
          3.1216351511944733,
          3.0598103347147383,
          2.9953220507337748,
          2.9870742980632348,
          2.934373976748284,
          2.8965933084509503,
          2.8862924639541587,
          2.859773414667124,
          2.8286511380387083,
          2.799110649597438,
          2.766734219062349,
          2.7450098880901947,
          2.7222547390128846,
          2.6867513731693218,
          2.6864175530214705,
          2.6738794264089516,
          2.6294738528863157,
          2.607358377718173,
          2.604694262291409,
          2.574461701972204,
          2.536738212739085,
          2.524435113911092,
          2.52127421571564,
          2.496557459057553,
          2.4931522679293927,
          2.4573576886601485,
          2.41405098476928,
          2.4115274234333834,
          2.374032145499169
         ],
         "yaxis": "y"
        },
        {
         "name": "Pop",
         "type": "scatter",
         "x": [
          "pop",
          "love",
          "voice",
          "time",
          "shes",
          "year",
          "vocal",
          "feel",
          "debut",
          "first",
          "singer",
          "producer",
          "even",
          "star",
          "make",
          "life",
          "way",
          "sings",
          "girl",
          "beat",
          "production",
          "guitar",
          "two",
          "take",
          "moment",
          "want",
          "also",
          "past",
          "back",
          "world",
          "something",
          "work",
          "single",
          "woman",
          "might",
          "lyric"
         ],
         "xaxis": "x2",
         "y": [
          4.762319965183439,
          3.4160129603223948,
          3.3753212039621543,
          3.338730380740367,
          3.2748877904217606,
          3.1101850691998667,
          3.079800167599735,
          2.994783933002285,
          2.9857620667013918,
          2.941686758185248,
          2.7308689420764525,
          2.5628943723131687,
          2.480153491220598,
          2.4637024896709576,
          2.410277896861029,
          2.3490245378543557,
          2.345862367326826,
          2.340486507249487,
          2.319899271770019,
          2.2841566117413143,
          2.2014857474111955,
          2.1911858645120237,
          2.179069875631799,
          2.1524074467093457,
          2.1228195756450976,
          2.1063391084464036,
          2.0918542675042393,
          2.0536624674055886,
          2.0494286234314822,
          2.030510189488999,
          1.9933109286097201,
          1.98232574817999,
          1.963026696503751,
          1.9458789805691858,
          1.9419869856054848,
          1.9231127017416723
         ],
         "yaxis": "y2"
        },
        {
         "name": "Rock",
         "type": "scatter",
         "x": [
          "band",
          "guitar",
          "rock",
          "time",
          "year",
          "first",
          "feel",
          "even",
          "love",
          "pop",
          "make",
          "life",
          "way",
          "group",
          "still",
          "get",
          "take",
          "also",
          "moment",
          "day",
          "vocal",
          "debut",
          "set",
          "two",
          "voice",
          "sings",
          "drum",
          "solo",
          "might",
          "live",
          "something",
          "melody",
          "lyric",
          "could",
          "much",
          "indie",
          "back",
          "around"
         ],
         "xaxis": "x3",
         "y": [
          11.145219930742151,
          6.717857384875672,
          5.920515045031608,
          5.365502629354054,
          5.231800752747598,
          4.963080138821006,
          4.9105423630160905,
          4.5533243687365,
          4.326484397635861,
          4.269561480732006,
          4.092244947645667,
          4.023700665179951,
          4.0168673720129675,
          3.9730172513609863,
          3.8718333751476472,
          3.838013654231388,
          3.764942243377072,
          3.74270055467854,
          3.696528824592446,
          3.683299934420241,
          3.6677447162205765,
          3.6495227750306096,
          3.632547453805333,
          3.6103493537557765,
          3.585763624238954,
          3.509973050019366,
          3.488614876048935,
          3.477729282352006,
          3.460122358584339,
          3.449232773121155,
          3.4463774809163263,
          3.382247721525169,
          3.3588374754693198,
          3.3402257389414847,
          3.3012040390169988,
          3.2931658019873775,
          3.2929505530231102,
          3.24025153490323
         ],
         "yaxis": "y3"
        },
        {
         "name": "Experimental",
         "type": "scatter",
         "x": [
          "feel",
          "time",
          "guitar",
          "year",
          "voice",
          "work",
          "band",
          "melody",
          "vocal",
          "first",
          "world",
          "ambient",
          "instrument",
          "pop",
          "life",
          "musician",
          "piano",
          "even",
          "debut",
          "composer",
          "moment",
          "make",
          "back",
          "something",
          "recording",
          "would",
          "take",
          "way",
          "two",
          "could",
          "piece",
          "across",
          "end",
          "come",
          "rock",
          "made",
          "experimental",
          "jazz"
         ],
         "xaxis": "x4",
         "y": [
          2.4316518637189986,
          2.302797596505437,
          2.273088926101215,
          2.152596655103828,
          2.11346704328751,
          2.0944867974237997,
          2.0660811600329616,
          1.9201364847177946,
          1.9016236869879624,
          1.8909654826099118,
          1.8406933473265799,
          1.8302344002245599,
          1.829486096599328,
          1.8208461435946757,
          1.7963571763782913,
          1.7839481552511343,
          1.7736745931018718,
          1.7139824973619053,
          1.6880231555206668,
          1.6611715872674224,
          1.6542039047618111,
          1.636122864844687,
          1.6324722368249165,
          1.6043204593739389,
          1.5908540339671116,
          1.5836082072922406,
          1.570433824073862,
          1.5663732225972749,
          1.5391507825475164,
          1.5024119726343246,
          1.4970508025682396,
          1.4706936669525272,
          1.4644641917728434,
          1.4642773647627554,
          1.4593924670861373,
          1.4566450300409035,
          1.445874935953937,
          1.4401709515946197
         ],
         "yaxis": "y4"
        },
        {
         "name": "Rap",
         "type": "scatter",
         "x": [
          "rap",
          "beat",
          "rapper",
          "feel",
          "make",
          "year",
          "time",
          "still",
          "get",
          "even",
          "life",
          "producer",
          "way",
          "work",
          "mike",
          "moment",
          "first",
          "last",
          "production",
          "much",
          "project",
          "could",
          "voice",
          "take",
          "sample",
          "love",
          "back",
          "drum",
          "two",
          "verse",
          "dont",
          "black",
          "say",
          "bar",
          "vocal",
          "flow"
         ],
         "xaxis": "x5",
         "y": [
          5.589617320456988,
          4.671477680613151,
          4.43604963660247,
          3.52667520746026,
          3.1310786264860035,
          3.1178868467187355,
          3.0240029487415985,
          3.0184638564202095,
          2.8588316848015287,
          2.8254007257777243,
          2.812782030778926,
          2.766921231116159,
          2.750434249970211,
          2.5628917952145907,
          2.4304689612005017,
          2.420707951115117,
          2.3976024090235617,
          2.363639774696303,
          2.3415856285142995,
          2.3348672853236963,
          2.3333119891708134,
          2.315507432116122,
          2.3082732238800188,
          2.306697215205506,
          2.2989250558171923,
          2.297498665452935,
          2.261591239100474,
          2.245913489481488,
          2.237391868586586,
          2.235351791283507,
          2.2169190050100362,
          2.2094234002679443,
          2.1965989049504384,
          2.177863809370134,
          2.1630036727266027,
          2.1489100145012183
         ],
         "yaxis": "y5"
        },
        {
         "name": "Folk",
         "type": "scatter",
         "x": [
          "country",
          "guitar",
          "folk",
          "time",
          "year",
          "way",
          "first",
          "love",
          "voice",
          "carter",
          "rock",
          "feel",
          "last",
          "make",
          "much",
          "two",
          "studio",
          "long",
          "light",
          "band",
          "work",
          "lyric",
          "sings",
          "never",
          "life",
          "debut",
          "moment",
          "instrument",
          "melody",
          "even",
          "acoustic",
          "something",
          "recording",
          "still",
          "thing",
          "also",
          "line"
         ],
         "xaxis": "x6",
         "y": [
          2.4976407479087017,
          2.2835356478980002,
          1.9076175440095189,
          1.8818443991237017,
          1.8131082319778227,
          1.7837384118554758,
          1.695382669607283,
          1.6625646908288985,
          1.652571812382826,
          1.5452048003000354,
          1.5440323821107695,
          1.4557366638750415,
          1.4490846280871847,
          1.3766594937182552,
          1.2939220053022975,
          1.2807338910664705,
          1.279419461640423,
          1.2594169988361463,
          1.2278500819561957,
          1.1872159713235664,
          1.1870586354892576,
          1.1869304457213812,
          1.1780977305604057,
          1.1767324349525081,
          1.1709791930065243,
          1.1663480069966685,
          1.1580648059863037,
          1.15307120149166,
          1.1493287564668082,
          1.1387986121065083,
          1.133468907776384,
          1.1012957606234681,
          1.0977977798501835,
          1.0975827523379371,
          1.0944051559082448,
          1.0916998939877796,
          1.0748954094778806
         ],
         "yaxis": "y6"
        },
        {
         "name": "Jazz",
         "type": "scatter",
         "x": [
          "jazz",
          "gendel",
          "score",
          "time",
          "take",
          "work",
          "piano",
          "two",
          "guitar",
          "feel",
          "saxophonist",
          "musician",
          "around",
          "first",
          "even",
          "solo",
          "instrument",
          "play",
          "make",
          "recorded",
          "moment",
          "bass",
          "recording",
          "get",
          "saxophone",
          "piece",
          "free",
          "live",
          "voice",
          "something",
          "playing",
          "player",
          "melody",
          "sander",
          "little",
          "band",
          "life",
          "decade"
         ],
         "xaxis": "x7",
         "y": [
          2.747130428831174,
          1.488231792481842,
          1.1957803743869577,
          1.1671095510545502,
          1.1480222487117224,
          1.1319173897011252,
          1.107859828523559,
          1.0573625704371474,
          1.0301220996269913,
          0.9983942278617883,
          0.9891035322413839,
          0.9883734891268648,
          0.9846658218350851,
          0.9813582230311981,
          0.9733171863880861,
          0.9731463235551969,
          0.9510721237782526,
          0.9387770680929477,
          0.9205528639506291,
          0.9048824321364158,
          0.89318546210184,
          0.8785399581544509,
          0.8739398625310364,
          0.8643092029363026,
          0.854531739931909,
          0.8477466551841462,
          0.8349718183919453,
          0.8284209618086128,
          0.8249642169927117,
          0.8150444357829763,
          0.8087117158256457,
          0.7833436887938833,
          0.7774595473340934,
          0.7679641095486681,
          0.765505784017109,
          0.7633950424098932,
          0.7618342621587625,
          0.7609481171789441
         ],
         "yaxis": "y7"
        },
        {
         "name": "Metal",
         "type": "scatter",
         "x": [
          "band",
          "metal",
          "death",
          "riff",
          "death metal",
          "everything",
          "guitar",
          "darkness",
          "first",
          "full",
          "another",
          "make",
          "rock",
          "nothing",
          "even",
          "lyric",
          "duo",
          "past",
          "year",
          "solo",
          "take",
          "thats",
          "something",
          "way",
          "feel",
          "could",
          "southern",
          "black",
          "two",
          "time",
          "also",
          "yet",
          "back",
          "find",
          "live",
          "return",
          "piece",
          "early"
         ],
         "xaxis": "x8",
         "y": [
          2.1918954412017517,
          1.86103915378915,
          1.0490787983668537,
          1.0283678366180993,
          0.9150218473073573,
          0.9122222632399085,
          0.9098369445498897,
          0.8553317531963909,
          0.7890318613674526,
          0.7491754147157382,
          0.7371729862428861,
          0.7260175272501088,
          0.7205551871419968,
          0.713276616525445,
          0.6874477905925644,
          0.6678624326875298,
          0.6497572087934652,
          0.6468808830572053,
          0.6448835255886574,
          0.6245617973316041,
          0.6157379089663582,
          0.6022871580009569,
          0.6018069471767741,
          0.6002573452176987,
          0.5906249125970875,
          0.5856475233151915,
          0.5841752401572157,
          0.5638318999730538,
          0.5498754970948962,
          0.5487793554823702,
          0.5338082146236176,
          0.5304269365464197,
          0.5284519028157633,
          0.5235106551741248,
          0.5195198598871184,
          0.5164439499107281,
          0.5121907598304933,
          0.4924013366367276
         ],
         "yaxis": "y8"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Electronic",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Pop",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.8671875,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Rock",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.734375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Experimental",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.6015625,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Rap",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.46875,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Folk",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.3359375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Jazz",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.203125,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Metal",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.0703125,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 2000,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Top Words by TF-IDF Score for Each Genre"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ]
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ]
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0,
          1
         ]
        },
        "xaxis7": {
         "anchor": "y7",
         "domain": [
          0,
          1
         ]
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.9296875,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.796875,
          0.8671875
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.6640625,
          0.734375
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.53125,
          0.6015625
         ]
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0.3984375,
          0.46875
         ]
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0.265625,
          0.3359375
         ]
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0.1328125,
          0.203125
         ]
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0,
          0.0703125
         ]
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"0da42c04-b540-4b53-9a79-c7f81ba5a183\" class=\"plotly-graph-div\" style=\"height:2000px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0da42c04-b540-4b53-9a79-c7f81ba5a183\")) {                    Plotly.newPlot(                        \"0da42c04-b540-4b53-9a79-c7f81ba5a183\",                        [{\"name\":\"Electronic\",\"x\":[\"feel\",\"producer\",\"vocal\",\"drum\",\"club\",\"pop\",\"time\",\"beat\",\"first\",\"even\",\"techno\",\"dance\",\"guitar\",\"ambient\",\"voice\",\"year\",\"house\",\"electronic\",\"band\",\"debut\",\"might\",\"make\",\"work\",\"synths\",\"synth\",\"long\",\"take\",\"world\",\"rhythm\",\"much\",\"space\",\"back\",\"set\",\"yet\",\"melody\",\"across\"],\"y\":[3.818589796209124,3.7023832839476034,3.4953721590324087,3.423403316959108,3.3832663181394724,3.308755107681813,3.261842193258468,3.1216351511944733,3.0598103347147383,2.9953220507337748,2.9870742980632348,2.934373976748284,2.8965933084509503,2.8862924639541587,2.859773414667124,2.8286511380387083,2.799110649597438,2.766734219062349,2.7450098880901947,2.7222547390128846,2.6867513731693218,2.6864175530214705,2.6738794264089516,2.6294738528863157,2.607358377718173,2.604694262291409,2.574461701972204,2.536738212739085,2.524435113911092,2.52127421571564,2.496557459057553,2.4931522679293927,2.4573576886601485,2.41405098476928,2.4115274234333834,2.374032145499169],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"name\":\"Pop\",\"x\":[\"pop\",\"love\",\"voice\",\"time\",\"shes\",\"year\",\"vocal\",\"feel\",\"debut\",\"first\",\"singer\",\"producer\",\"even\",\"star\",\"make\",\"life\",\"way\",\"sings\",\"girl\",\"beat\",\"production\",\"guitar\",\"two\",\"take\",\"moment\",\"want\",\"also\",\"past\",\"back\",\"world\",\"something\",\"work\",\"single\",\"woman\",\"might\",\"lyric\"],\"y\":[4.762319965183439,3.4160129603223948,3.3753212039621543,3.338730380740367,3.2748877904217606,3.1101850691998667,3.079800167599735,2.994783933002285,2.9857620667013918,2.941686758185248,2.7308689420764525,2.5628943723131687,2.480153491220598,2.4637024896709576,2.410277896861029,2.3490245378543557,2.345862367326826,2.340486507249487,2.319899271770019,2.2841566117413143,2.2014857474111955,2.1911858645120237,2.179069875631799,2.1524074467093457,2.1228195756450976,2.1063391084464036,2.0918542675042393,2.0536624674055886,2.0494286234314822,2.030510189488999,1.9933109286097201,1.98232574817999,1.963026696503751,1.9458789805691858,1.9419869856054848,1.9231127017416723],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"name\":\"Rock\",\"x\":[\"band\",\"guitar\",\"rock\",\"time\",\"year\",\"first\",\"feel\",\"even\",\"love\",\"pop\",\"make\",\"life\",\"way\",\"group\",\"still\",\"get\",\"take\",\"also\",\"moment\",\"day\",\"vocal\",\"debut\",\"set\",\"two\",\"voice\",\"sings\",\"drum\",\"solo\",\"might\",\"live\",\"something\",\"melody\",\"lyric\",\"could\",\"much\",\"indie\",\"back\",\"around\"],\"y\":[11.145219930742151,6.717857384875672,5.920515045031608,5.365502629354054,5.231800752747598,4.963080138821006,4.9105423630160905,4.5533243687365,4.326484397635861,4.269561480732006,4.092244947645667,4.023700665179951,4.0168673720129675,3.9730172513609863,3.8718333751476472,3.838013654231388,3.764942243377072,3.74270055467854,3.696528824592446,3.683299934420241,3.6677447162205765,3.6495227750306096,3.632547453805333,3.6103493537557765,3.585763624238954,3.509973050019366,3.488614876048935,3.477729282352006,3.460122358584339,3.449232773121155,3.4463774809163263,3.382247721525169,3.3588374754693198,3.3402257389414847,3.3012040390169988,3.2931658019873775,3.2929505530231102,3.24025153490323],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"name\":\"Experimental\",\"x\":[\"feel\",\"time\",\"guitar\",\"year\",\"voice\",\"work\",\"band\",\"melody\",\"vocal\",\"first\",\"world\",\"ambient\",\"instrument\",\"pop\",\"life\",\"musician\",\"piano\",\"even\",\"debut\",\"composer\",\"moment\",\"make\",\"back\",\"something\",\"recording\",\"would\",\"take\",\"way\",\"two\",\"could\",\"piece\",\"across\",\"end\",\"come\",\"rock\",\"made\",\"experimental\",\"jazz\"],\"y\":[2.4316518637189986,2.302797596505437,2.273088926101215,2.152596655103828,2.11346704328751,2.0944867974237997,2.0660811600329616,1.9201364847177946,1.9016236869879624,1.8909654826099118,1.8406933473265799,1.8302344002245599,1.829486096599328,1.8208461435946757,1.7963571763782913,1.7839481552511343,1.7736745931018718,1.7139824973619053,1.6880231555206668,1.6611715872674224,1.6542039047618111,1.636122864844687,1.6324722368249165,1.6043204593739389,1.5908540339671116,1.5836082072922406,1.570433824073862,1.5663732225972749,1.5391507825475164,1.5024119726343246,1.4970508025682396,1.4706936669525272,1.4644641917728434,1.4642773647627554,1.4593924670861373,1.4566450300409035,1.445874935953937,1.4401709515946197],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"name\":\"Rap\",\"x\":[\"rap\",\"beat\",\"rapper\",\"feel\",\"make\",\"year\",\"time\",\"still\",\"get\",\"even\",\"life\",\"producer\",\"way\",\"work\",\"mike\",\"moment\",\"first\",\"last\",\"production\",\"much\",\"project\",\"could\",\"voice\",\"take\",\"sample\",\"love\",\"back\",\"drum\",\"two\",\"verse\",\"dont\",\"black\",\"say\",\"bar\",\"vocal\",\"flow\"],\"y\":[5.589617320456988,4.671477680613151,4.43604963660247,3.52667520746026,3.1310786264860035,3.1178868467187355,3.0240029487415985,3.0184638564202095,2.8588316848015287,2.8254007257777243,2.812782030778926,2.766921231116159,2.750434249970211,2.5628917952145907,2.4304689612005017,2.420707951115117,2.3976024090235617,2.363639774696303,2.3415856285142995,2.3348672853236963,2.3333119891708134,2.315507432116122,2.3082732238800188,2.306697215205506,2.2989250558171923,2.297498665452935,2.261591239100474,2.245913489481488,2.237391868586586,2.235351791283507,2.2169190050100362,2.2094234002679443,2.1965989049504384,2.177863809370134,2.1630036727266027,2.1489100145012183],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"name\":\"Folk\",\"x\":[\"country\",\"guitar\",\"folk\",\"time\",\"year\",\"way\",\"first\",\"love\",\"voice\",\"carter\",\"rock\",\"feel\",\"last\",\"make\",\"much\",\"two\",\"studio\",\"long\",\"light\",\"band\",\"work\",\"lyric\",\"sings\",\"never\",\"life\",\"debut\",\"moment\",\"instrument\",\"melody\",\"even\",\"acoustic\",\"something\",\"recording\",\"still\",\"thing\",\"also\",\"line\"],\"y\":[2.4976407479087017,2.2835356478980002,1.9076175440095189,1.8818443991237017,1.8131082319778227,1.7837384118554758,1.695382669607283,1.6625646908288985,1.652571812382826,1.5452048003000354,1.5440323821107695,1.4557366638750415,1.4490846280871847,1.3766594937182552,1.2939220053022975,1.2807338910664705,1.279419461640423,1.2594169988361463,1.2278500819561957,1.1872159713235664,1.1870586354892576,1.1869304457213812,1.1780977305604057,1.1767324349525081,1.1709791930065243,1.1663480069966685,1.1580648059863037,1.15307120149166,1.1493287564668082,1.1387986121065083,1.133468907776384,1.1012957606234681,1.0977977798501835,1.0975827523379371,1.0944051559082448,1.0916998939877796,1.0748954094778806],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"name\":\"Jazz\",\"x\":[\"jazz\",\"gendel\",\"score\",\"time\",\"take\",\"work\",\"piano\",\"two\",\"guitar\",\"feel\",\"saxophonist\",\"musician\",\"around\",\"first\",\"even\",\"solo\",\"instrument\",\"play\",\"make\",\"recorded\",\"moment\",\"bass\",\"recording\",\"get\",\"saxophone\",\"piece\",\"free\",\"live\",\"voice\",\"something\",\"playing\",\"player\",\"melody\",\"sander\",\"little\",\"band\",\"life\",\"decade\"],\"y\":[2.747130428831174,1.488231792481842,1.1957803743869577,1.1671095510545502,1.1480222487117224,1.1319173897011252,1.107859828523559,1.0573625704371474,1.0301220996269913,0.9983942278617883,0.9891035322413839,0.9883734891268648,0.9846658218350851,0.9813582230311981,0.9733171863880861,0.9731463235551969,0.9510721237782526,0.9387770680929477,0.9205528639506291,0.9048824321364158,0.89318546210184,0.8785399581544509,0.8739398625310364,0.8643092029363026,0.854531739931909,0.8477466551841462,0.8349718183919453,0.8284209618086128,0.8249642169927117,0.8150444357829763,0.8087117158256457,0.7833436887938833,0.7774595473340934,0.7679641095486681,0.765505784017109,0.7633950424098932,0.7618342621587625,0.7609481171789441],\"type\":\"scatter\",\"xaxis\":\"x7\",\"yaxis\":\"y7\"},{\"name\":\"Metal\",\"x\":[\"band\",\"metal\",\"death\",\"riff\",\"death metal\",\"everything\",\"guitar\",\"darkness\",\"first\",\"full\",\"another\",\"make\",\"rock\",\"nothing\",\"even\",\"lyric\",\"duo\",\"past\",\"year\",\"solo\",\"take\",\"thats\",\"something\",\"way\",\"feel\",\"could\",\"southern\",\"black\",\"two\",\"time\",\"also\",\"yet\",\"back\",\"find\",\"live\",\"return\",\"piece\",\"early\"],\"y\":[2.1918954412017517,1.86103915378915,1.0490787983668537,1.0283678366180993,0.9150218473073573,0.9122222632399085,0.9098369445498897,0.8553317531963909,0.7890318613674526,0.7491754147157382,0.7371729862428861,0.7260175272501088,0.7205551871419968,0.713276616525445,0.6874477905925644,0.6678624326875298,0.6497572087934652,0.6468808830572053,0.6448835255886574,0.6245617973316041,0.6157379089663582,0.6022871580009569,0.6018069471767741,0.6002573452176987,0.5906249125970875,0.5856475233151915,0.5841752401572157,0.5638318999730538,0.5498754970948962,0.5487793554823702,0.5338082146236176,0.5304269365464197,0.5284519028157633,0.5235106551741248,0.5195198598871184,0.5164439499107281,0.5121907598304933,0.4924013366367276],\"type\":\"scatter\",\"xaxis\":\"x8\",\"yaxis\":\"y8\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.9296875,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.796875,0.8671875]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.6640625,0.734375]},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.0,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.53125,0.6015625]},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,1.0]},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.3984375,0.46875]},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.0,1.0]},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.265625,0.3359375]},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.0,1.0]},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.1328125,0.203125]},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.0,1.0]},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.0,0.0703125]},\"annotations\":[{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Electronic\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Pop\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.8671875,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Rock\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.734375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Experimental\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6015625,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Rap\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.46875,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Folk\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.3359375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Jazz\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.203125,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Metal\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.0703125,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Top Words by TF-IDF Score for Each Genre\"},\"height\":2000,\"width\":1000,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0da42c04-b540-4b53-9a79-c7f81ba5a183');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create visualisation\n",
    "genres = [\"Electronic\", \"Pop\", \"Rock\", \"Experimental\", \"Rap\", \"Folk\", \"Jazz\", \"Metal\"]\n",
    "\n",
    "fig = make_subplots(rows=len(genres), cols=1, shared_yaxes=True, subplot_titles=genres)\n",
    "\n",
    "for i, genre in enumerate(genres):\n",
    "    genre_df = important_words_by_genre[genre]  # This assumes each genre's words & scores are in a dictionary\n",
    "    genre_df = genre_df[genre_df['word'].apply(lambda x: len(x) > 2)]\n",
    "    fig.add_trace(go.Scatter(x=genre_df['word'], y=genre_df['tfidf_score'], name=genre), row=i+1, col=1)\n",
    "\n",
    "fig.update_annotations(font_size=12)\n",
    "\n",
    "fig.update_layout(height=2000, width=1000, title_text=\"Top Words by TF-IDF Score for Each Genre\", showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5983583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add genre tags and save output\n",
    "pop['genre']= 'pop'\n",
    "electronic['genre']='electronic'\n",
    "rock['genre']= 'rock'\n",
    "experimental['genre']= 'experimental'\n",
    "rap['genre']= 'rap'\n",
    "folk['genre']= 'folk'\n",
    "jazz['genre']= 'jazz'\n",
    "metal['genre']= 'metal'\n",
    "\n",
    "data_frames = [electronic, pop, rock, experimental, rap, folk, jazz, metal]\n",
    " \n",
    "full_tfidf = pd.concat(data_frames).reset_index()\n",
    "full_tfidf = full_tfidf.iloc[:,1:]\n",
    "\n",
    "full_tfidf.to_csv('/Users/simoncrouch/Desktop/analysis_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2fdd6",
   "metadata": {},
   "source": [
    "### Write up Analysis\n",
    "\n",
    "Score Differences\n",
    "\n",
    "Genres have different words. Which voerlap, which are more unique?\n",
    "\n",
    "The variation in TF-IDF score ranges between genres like Metal and Rock suggests differences in word importance within the genres' vocabularies, reflecting how distinct or specialized the language is in each genre's reviews. Hereâ€™s how to interpret this:\n",
    "\n",
    "    Higher Maximum TF-IDF in Rock: If Rock has a maximum TF-IDF score of 12 for its top word while Metal tops out at 2.5, this implies that Rock reviews use a few highly distinctive words more consistently across reviews. In other words, Rock reviews may have certain words that are both specific to Rock and frequently appear, which increases their TF-IDF scores. Metal reviews, on the other hand, might have less concentrated usage of specific words.\n",
    "\n",
    "    Score Range and Term Specificity: The broader score range in Rock (12 to 2.5) suggests that there is a clearer hierarchy of word importance; certain terms stand out as particularly characteristic of Rock reviews. For Metal, the narrower range (2.5 to 0.3) indicates that while certain words are somewhat distinctive, they donâ€™t dominate the genreâ€™s vocabulary as much, which could mean Metal reviews use more varied or genre-neutral language.\n",
    "\n",
    "    Inter-Genre Comparison: The TF-IDF scores are relative to each dataset's context, meaning that the vectorizer has calculated importance based on the frequency and exclusivity of words within each genre's reviews. Consequently, words in Rock reviews might have higher scores because they are more unusual when compared to other genres, while Metal might share more vocabulary with other genres, diluting its TF-IDF scores.\n",
    "\n",
    "This analysis highlights how genre language varies in specificity and distinctiveness. You can interpret TF-IDF scores as a measure of how closely reviews for a genre gravitate around a unique vocabulary, helping to distinguish genre-specific language patterns.\n",
    "\n",
    "THIS SECTION PROVING THAT DIFFERENT LANGUAGE IS USED FOR DIFFERENT GENRES\n",
    "NEXT SECTION, SO CAN WE USE THAT TO DETERMINE WHAT GENRE A REVIEW IS FOR BASED ON ITS TEXT\n",
    "THEN CAN WE PREDICT THE SCORE BASED ON LANGUAGE USED (If I can be bothered)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9681d39",
   "metadata": {},
   "source": [
    "High-Scoring Terms by Genre:\n",
    "\n",
    "    Electronic: Words like \"producer,\" \"synth,\" \"ambient,\" and \"techno\" have high TF-IDF scores, highlighting the genre's focus on production elements and electronic subgenres.\n",
    "    Rock: \"Band,\" \"guitar,\" and \"rock\" are central, indicating the importance of traditional band setups and instruments.\n",
    "    Rap: \"Rap,\" \"beat,\" and \"rapper\" emphasize rhythmic and vocal elements typical in this genre.\n",
    "    Jazz: \"Saxophone,\" \"solo,\" and \"piano\" appear, reflecting jazz's reliance on individual instruments and improvisation.\n",
    "    Metal: Terms like \"riff,\" \"death,\" and \"darkness\" reveal themes of intensity and dark tonality.\n",
    "\n",
    "Common Words Across Genres:\n",
    "\n",
    "    Words like \"album,\" \"song,\" and \"music\" score highly across most genres, as theyâ€™re core to discussing any music form.\n",
    "\n",
    "Genre-Specific Language Patterns:\n",
    "\n",
    "    This data suggests that while certain themes (like emotion, time, and creation) are universal, genres diverge in their lexicon. For instance, rock and metal often share references to instruments, but metal includes darker thematic words.\n",
    "\n",
    "Potential Applications:\n",
    "\n",
    "    Genre Prediction: These TF-IDF scores can serve as features in a classification model to predict the genre of a review based on word frequency.\n",
    "    \n",
    "    \n",
    "Genre-Defining Terms:\n",
    "\n",
    "\n",
    "Rock has the strongest genre-specific vocabulary, with \"band\" having the highest TF-IDF score (11.15) across all genres\n",
    "Rap shows strong association with its core elements (\"rap\", \"beat\", \"rapper\")\n",
    "Metal focuses on specific subgenres (\"death metal\") and technical elements (\"riff\")\n",
    "\n",
    "\n",
    "Instrumental Focus:\n",
    "\n",
    "\n",
    "Guitar appears prominently in rock (6.72), folk (2.28), and appears across multiple genres\n",
    "Electronic music emphasizes production elements (\"producer\", \"drum\", \"synth\")\n",
    "Jazz highlights specific instruments (\"piano\", \"saxophone\", \"bass\")\n",
    "\n",
    "\n",
    "Vocal/Lyrical Elements:\n",
    "\n",
    "\n",
    "Pop emphasizes vocals and emotional content (\"voice\", \"love\", \"she's\")\n",
    "Rap focuses on delivery (\"flow\", \"verse\", \"bar\")\n",
    "Metal pays attention to lyrical themes (\"darkness\")\n",
    "\n",
    "\n",
    "Common Ground:\n",
    "\n",
    "\n",
    "\"Time\" appears as an important term across multiple genres\n",
    "\"Feel\" is significant in electronic, rap, and rock\n",
    "\"Voice\" appears prominently in pop and crosses over to other genres\n",
    "\n",
    "\n",
    "Production Elements:\n",
    "\n",
    "\n",
    "Electronic music has strong associations with production terms (\"producer\", \"ambient\", \"synth\")\n",
    "Rap emphasizes beats and production (\"beat\", \"producer\", \"sample\")\n",
    "Folk and rock tend to use more traditional instrumental terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a46d9",
   "metadata": {},
   "source": [
    "# Can review be used to predict genre?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e9718385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>feel</td>\n",
       "      <td>3.818590</td>\n",
       "      <td>electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>producer</td>\n",
       "      <td>3.702383</td>\n",
       "      <td>electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>vocal</td>\n",
       "      <td>3.495372</td>\n",
       "      <td>electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>drum</td>\n",
       "      <td>3.423403</td>\n",
       "      <td>electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>club</td>\n",
       "      <td>3.383266</td>\n",
       "      <td>electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>35</td>\n",
       "      <td>find</td>\n",
       "      <td>0.523511</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>36</td>\n",
       "      <td>live</td>\n",
       "      <td>0.519520</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>37</td>\n",
       "      <td>return</td>\n",
       "      <td>0.516444</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>38</td>\n",
       "      <td>piece</td>\n",
       "      <td>0.512191</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>39</td>\n",
       "      <td>early</td>\n",
       "      <td>0.492401</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index      word  tfidf_score       genre\n",
       "0        2      feel     3.818590  electronic\n",
       "1        3  producer     3.702383  electronic\n",
       "2        4     vocal     3.495372  electronic\n",
       "3        5      drum     3.423403  electronic\n",
       "4        6      club     3.383266  electronic\n",
       "..     ...       ...          ...         ...\n",
       "292     35      find     0.523511       metal\n",
       "293     36      live     0.519520       metal\n",
       "294     37    return     0.516444       metal\n",
       "295     38     piece     0.512191       metal\n",
       "296     39     early     0.492401       metal\n",
       "\n",
       "[297 rows x 4 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "651eceb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.824964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1.097798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.493152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1.057363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.562894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2.191895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.654204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.893185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.777460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3.018464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tfidf_score\n",
       "249     0.824964\n",
       "216     1.097798\n",
       "31      2.493152\n",
       "228     1.057363\n",
       "47      2.562894\n",
       "..           ...\n",
       "259     2.191895\n",
       "130     1.654204\n",
       "241     0.893185\n",
       "253     0.777460\n",
       "155     3.018464\n",
       "\n",
       "[237 rows x 1 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "14c4d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Score</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ten years after his big solo debut the uk prod...</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>7.3</td>\n",
       "      <td>[ten, year, big, solo, debut, , uk, producer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>riding the success of singles like that and no...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>7.2</td>\n",
       "      <td>[riding, success, single, â€œ, like, , â€, â€œ, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on their debut collaboration the beatmaker and...</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[debut, collaboration, , beatmaker, drummer, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the philly groups second live album is a celeb...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>7.9</td>\n",
       "      <td>[philly, group, â€™, second, live, album, celebr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>on her third album the uk singersongwriter sou...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[third, album, , uk, singersongwriter, sound, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>the reggae veterans new studio album doesnt ma...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>6.7</td>\n",
       "      <td>[reggae, veteran, â€™, new, studio, album, â€™, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>in diaphanous compositions like color field pa...</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[diaphanous, composition, like, color, field, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>the singaporean bands new album showcases a pu...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>7.2</td>\n",
       "      <td>[singaporean, band, â€™, new, album, showcase, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>each sunday pitchfork takes an indepth look at...</td>\n",
       "      <td>Folk</td>\n",
       "      <td>9.6</td>\n",
       "      <td>[sunday, , pitchfork, take, indepth, look, sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>this 1977 album was obscure in its day but it ...</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[1977, album, obscure, day, , spawned, fervent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text         Genre  Score  \\\n",
       "0    ten years after his big solo debut the uk prod...    Electronic    7.3   \n",
       "1    riding the success of singles like that and no...           Pop    7.2   \n",
       "2    on their debut collaboration the beatmaker and...    Electronic    7.8   \n",
       "3    the philly groups second live album is a celeb...          Rock    7.9   \n",
       "5    on her third album the uk singersongwriter sou...           Pop    8.5   \n",
       "..                                                 ...           ...    ...   \n",
       "800  the reggae veterans new studio album doesnt ma...           Pop    6.7   \n",
       "801  in diaphanous compositions like color field pa...  Experimental    7.5   \n",
       "802  the singaporean bands new album showcases a pu...          Rock    7.2   \n",
       "803  each sunday pitchfork takes an indepth look at...          Folk    9.6   \n",
       "804  this 1977 album was obscure in its day but it ...          Jazz    8.9   \n",
       "\n",
       "                                                tokens  \n",
       "0    [ten, year, big, solo, debut, , uk, producer, ...  \n",
       "1    [riding, success, single, â€œ, like, , â€, â€œ, not...  \n",
       "2    [debut, collaboration, , beatmaker, drummer, s...  \n",
       "3    [philly, group, â€™, second, live, album, celebr...  \n",
       "5    [third, album, , uk, singersongwriter, sound, ...  \n",
       "..                                                 ...  \n",
       "800  [reggae, veteran, â€™, new, studio, album, â€™, ma...  \n",
       "801  [diaphanous, composition, like, color, field, ...  \n",
       "802  [singaporean, band, â€™, new, album, showcase, p...  \n",
       "803  [sunday, , pitchfork, take, indepth, look, sig...  \n",
       "804  [1977, album, obscure, day, , spawned, fervent...  \n",
       "\n",
       "[795 rows x 4 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bbe11",
   "metadata": {},
   "source": [
    "Fit Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4c256de9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vectorize entire body of text\n",
    "X_tfidf = vectorizer.fit_transform(df['Text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Create a DataFrame to hold the TF-IDF scores\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "348f2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add target column to tf-idf dataframe\n",
    "tfidf_df['Genre'] = df['Genre'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6fefd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df.drop(columns=['Genre'])  # Feature columns (TF-IDF scores)\n",
    "y = tfidf_df['Genre']                 # Target column (genre)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fc608a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5317a3a",
   "metadata": {},
   "source": [
    "apply classification report instead then don't need to load in accuracy at start. same for next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ead6b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6289308176100629\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db42cad",
   "metadata": {},
   "source": [
    "Training on each genre separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "79883a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold each genre's TF-IDF DataFrame\n",
    "tfidf_dfs = []\n",
    "\n",
    "for genre in df['Genre'].unique():\n",
    "    # Filter text by genre\n",
    "    genre_text = df[df['Genre'] == genre]['Text']\n",
    "    # Fit and transform the vectorizer on this genre's text data\n",
    "    X_genre_tfidf = vectorizer.fit_transform(genre_text)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    # Create a DataFrame with TF-IDF scores and add the genre as a column\n",
    "    genre_tfidf_df = pd.DataFrame(X_genre_tfidf.toarray(), columns=feature_names)\n",
    "    genre_tfidf_df['Genre'] = genre  # Add genre label as a new column\n",
    "    # Append to the list\n",
    "    tfidf_dfs.append(genre_tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7f265eeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Concatenate all genre-specific TF-IDF DataFrames into one\n",
    "combined_tfidf_df = pd.concat(tfidf_dfs, ignore_index=True)\n",
    "# Due to different word lists between genres the dataframe contains NaN values which must be converted to 0s\n",
    "combined_tfidf_df = combined_tfidf_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2c92739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6540880503144654\n"
     ]
    }
   ],
   "source": [
    "X = combined_tfidf_df.drop(columns=['Genre'])  # Feature columns (TF-IDF scores)\n",
    "y = combined_tfidf_df['Genre']                 # Target column (genre)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9b6df",
   "metadata": {},
   "source": [
    "Accuracy has improved! Now hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3912f",
   "metadata": {},
   "source": [
    "Fit naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43124a0",
   "metadata": {},
   "source": [
    "NB using whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e00dbf7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Score</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ten years after his big solo debut the uk prod...</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>7.3</td>\n",
       "      <td>[ten, year, big, solo, debut, , uk, producer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>riding the success of singles like that and no...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>7.2</td>\n",
       "      <td>[riding, success, single, â€œ, like, , â€, â€œ, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on their debut collaboration the beatmaker and...</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[debut, collaboration, , beatmaker, drummer, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the philly groups second live album is a celeb...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>7.9</td>\n",
       "      <td>[philly, group, â€™, second, live, album, celebr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>on her third album the uk singersongwriter sou...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[third, album, , uk, singersongwriter, sound, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>the reggae veterans new studio album doesnt ma...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>6.7</td>\n",
       "      <td>[reggae, veteran, â€™, new, studio, album, â€™, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>in diaphanous compositions like color field pa...</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[diaphanous, composition, like, color, field, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>the singaporean bands new album showcases a pu...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>7.2</td>\n",
       "      <td>[singaporean, band, â€™, new, album, showcase, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>each sunday pitchfork takes an indepth look at...</td>\n",
       "      <td>Folk</td>\n",
       "      <td>9.6</td>\n",
       "      <td>[sunday, , pitchfork, take, indepth, look, sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>this 1977 album was obscure in its day but it ...</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[1977, album, obscure, day, , spawned, fervent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text         Genre  Score  \\\n",
       "0    ten years after his big solo debut the uk prod...    Electronic    7.3   \n",
       "1    riding the success of singles like that and no...           Pop    7.2   \n",
       "2    on their debut collaboration the beatmaker and...    Electronic    7.8   \n",
       "3    the philly groups second live album is a celeb...          Rock    7.9   \n",
       "5    on her third album the uk singersongwriter sou...           Pop    8.5   \n",
       "..                                                 ...           ...    ...   \n",
       "800  the reggae veterans new studio album doesnt ma...           Pop    6.7   \n",
       "801  in diaphanous compositions like color field pa...  Experimental    7.5   \n",
       "802  the singaporean bands new album showcases a pu...          Rock    7.2   \n",
       "803  each sunday pitchfork takes an indepth look at...          Folk    9.6   \n",
       "804  this 1977 album was obscure in its day but it ...          Jazz    8.9   \n",
       "\n",
       "                                                tokens  \n",
       "0    [ten, year, big, solo, debut, , uk, producer, ...  \n",
       "1    [riding, success, single, â€œ, like, , â€, â€œ, not...  \n",
       "2    [debut, collaboration, , beatmaker, drummer, s...  \n",
       "3    [philly, group, â€™, second, live, album, celebr...  \n",
       "5    [third, album, , uk, singersongwriter, sound, ...  \n",
       "..                                                 ...  \n",
       "800  [reggae, veteran, â€™, new, studio, album, â€™, ma...  \n",
       "801  [diaphanous, composition, like, color, field, ...  \n",
       "802  [singaporean, band, â€™, new, album, showcase, p...  \n",
       "803  [sunday, , pitchfork, take, indepth, look, sig...  \n",
       "804  [1977, album, obscure, day, , spawned, fervent...  \n",
       "\n",
       "[795 rows x 4 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6d33bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Electronic       0.64      0.50      0.56        28\n",
      "Experimental       0.00      0.00      0.00        15\n",
      "        Folk       1.00      0.10      0.18        10\n",
      "        Jazz       1.00      0.17      0.29         6\n",
      "       Metal       0.00      0.00      0.00         2\n",
      "         Pop       0.80      0.33      0.47        24\n",
      "         Rap       0.91      0.81      0.86        26\n",
      "        Rock       0.46      0.98      0.63        48\n",
      "\n",
      "    accuracy                           0.58       159\n",
      "   macro avg       0.60      0.36      0.37       159\n",
      "weighted avg       0.62      0.58      0.52       159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorize entire body of text\n",
    "X_tfidf = vectorizer.fit_transform(df['Text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Create a DataFrame to hold the TF-IDF scores\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "X = tfidf_df  # All text data\n",
    "y = df['Genre']  # All genre labels\n",
    "\n",
    "# Encode genre labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=12,\n",
    "    stratify=y_encoded  # Maintain genre distribution in train/test sets\n",
    ")\n",
    "    \n",
    "# Train the classifier\n",
    "clf = ComplementNB()\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "    \n",
    "# Print evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, \n",
    "                            target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79de1f",
   "metadata": {},
   "source": [
    "NB when tf-idf has been trained on each genre seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f08dc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold each genre's TF-IDF DataFrame\n",
    "tfidf_dfs = []\n",
    "\n",
    "for genre in df['Genre'].unique():\n",
    "    # Filter text by genre\n",
    "    genre_text = df[df['Genre'] == genre]['Text']\n",
    "    # Fit and transform the vectorizer on this genre's text data\n",
    "    X_genre_tfidf = vectorizer.fit_transform(genre_text)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    # Create a DataFrame with TF-IDF scores and add the genre as a column\n",
    "    genre_tfidf_df = pd.DataFrame(X_genre_tfidf.toarray(), columns=feature_names)\n",
    "    genre_tfidf_df['Genre'] = genre  # Add genre label as a new column\n",
    "    # Append to the list\n",
    "    tfidf_dfs.append(genre_tfidf_df)\n",
    "\n",
    "# Concatenate all genre-specific TF-IDF DataFrames into one\n",
    "combined_tfidf_df = pd.concat(tfidf_dfs, ignore_index=True)\n",
    "# Due to different word lists between genres the dataframe contains NaN values which must be converted to 0s\n",
    "combined_tfidf_df = combined_tfidf_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7823b9ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Electronic       0.92      0.86      0.89        28\n",
      "Experimental       1.00      0.80      0.89        15\n",
      "        Folk       1.00      0.20      0.33        10\n",
      "        Jazz       1.00      0.67      0.80         6\n",
      "       Metal       0.00      0.00      0.00         2\n",
      "         Pop       0.96      0.92      0.94        24\n",
      "         Rap       1.00      0.96      0.98        26\n",
      "        Rock       0.70      0.98      0.82        48\n",
      "\n",
      "    accuracy                           0.86       159\n",
      "   macro avg       0.82      0.67      0.71       159\n",
      "weighted avg       0.88      0.86      0.84       159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = combined_tfidf_df.drop(columns=['Genre'])  # Feature columns (TF-IDF scores)\n",
    "y = combined_tfidf_df['Genre']                 # Target column (genre)\n",
    "\n",
    "# Encode genre labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=12,\n",
    "    stratify=y_encoded  # Maintain genre distribution in train/test sets\n",
    ")\n",
    "    \n",
    "# Train the classifier\n",
    "clf = ComplementNB()\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "    \n",
    "# Print evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, \n",
    "                            target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c7498",
   "metadata": {},
   "source": [
    "Analysis\n",
    "\n",
    "why ComplementNB might outperform Naive Bayes and Random Forest for your music genre classification task.\n",
    "\n",
    "Class Imbalance Impact:\n",
    "\n",
    "\n",
    "Your data shows significant imbalance:\n",
    "\n",
    "Rock: 48 samples\n",
    "Electronic: 28 samples\n",
    "Rap: 26 samples\n",
    "Pop: 24 samples\n",
    "Experimental: 15 samples\n",
    "Folk: 10 samples\n",
    "Jazz: 6 samples\n",
    "Metal: 2 samples\n",
    "\n",
    "\n",
    "ComplementNB is specifically designed for imbalanced datasets:\n",
    "\n",
    "It estimates parameters using samples from all classes except the one being modeled\n",
    "This helps with the small classes (Metal, Jazz, Folk) by using more data for parameter estimation\n",
    "Regular Naive Bayes would struggle with small classes as it has less data to estimate parameters\n",
    "This explains why Metal (2 samples) got 0% and Folk (10 samples) got low recall but perfect precision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Text Classification Characteristics:\n",
    "\n",
    "\n",
    "ComplementNB advantages:\n",
    "\n",
    "Better handles the \"long tail\" of rare words in music reviews\n",
    "More robust to vocabulary differences between genres\n",
    "Less sensitive to dominant classes overwhelming minority classes\n",
    "\n",
    "\n",
    "Regular Naive Bayes limitations:\n",
    "\n",
    "More sensitive to class imbalance\n",
    "Can be overwhelmed by dominant classes (Rock in your case)\n",
    "May overfit to specific words in small classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Model Performance Analysis:\n",
    "\n",
    "\n",
    "Strengths shown in results:\n",
    "\n",
    "High precision across most genres (many 1.00)\n",
    "Strong performance on medium-sized classes (Rap: 0.98 F1, Pop: 0.94 F1)\n",
    "Good balance for Electronic (0.89 F1)\n",
    "\n",
    "\n",
    "Challenges shown:\n",
    "\n",
    "Metal: Complete failure (0.00 across board) due to tiny sample size\n",
    "Folk: Low recall (0.20) despite perfect precision\n",
    "Rock: Lower precision (0.70) but high recall (0.98)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Why Random Forest Performed Middle:\n",
    "\n",
    "\n",
    "Random Forest characteristics:\n",
    "\n",
    "Good at handling non-linear relationships\n",
    "Can capture complex patterns in text\n",
    "But may struggle with:\n",
    "\n",
    "High-dimensional sparse data (typical in text)\n",
    "Very imbalanced classes\n",
    "Limited training data for some classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d739ed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8553459119496856\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbe137",
   "metadata": {},
   "source": [
    "Hyperparameter tuning\n",
    "\n",
    "Pipeline Integration: Combines the vectorizer and classifier into a single pipeline, ensuring proper feature transformation during cross-validation.\n",
    "Key Parameters to Tune:\n",
    "\n",
    "For TfidfVectorizer:\n",
    "\n",
    "min_df: Try different minimum document frequencies\n",
    "ngram_range: Test different n-gram combinations\n",
    "max_features: Limit vocabulary size\n",
    "norm: Try different normalization schemes\n",
    "\n",
    "\n",
    "For ComplementNB:\n",
    "\n",
    "alpha: Smoothing parameter\n",
    "norm: Whether to normalize weight vectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cross-validation with GridSearchCV ensures robust parameter selection\n",
    "F1-weighted scoring metric accounts for potential class imbalance\n",
    "\n",
    "You can run this code as is, or modify the param_grid based on your specific needs. Some additional suggestions:\n",
    "\n",
    "If runtime is a concern, you could:\n",
    "\n",
    "Reduce the parameter grid size\n",
    "Use RandomizedSearchCV instead of GridSearchCV\n",
    "Reduce the number of cross-validation folds\n",
    "\n",
    "\n",
    "If memory is a concern, you might want to:\n",
    "\n",
    "Start with a smaller max_features range\n",
    "Limit the ngram_range to (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "662369c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3400582306.py, line 56)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [191]\u001b[0;36m\u001b[0m\n\u001b[0;31m    tfidf_dfs = []\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL PIPELINE WITHOUT TFIDF ON INDIVIDUAL GENRE TEXT\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_genre_specific_features(df, vectorizer, n_top_features=10):\n",
    "    \"\"\"Analyze top features for each genre after fitting the vectorizer once\"\"\"\n",
    "    genre_features = {}\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for genre in df['Genre'].unique():\n",
    "        # Filter text by genre\n",
    "        genre_mask = df['Genre'] == genre\n",
    "        genre_tfidf = vectorizer.transform(df[genre_mask]['Text'])\n",
    "        \n",
    "        # Calculate average TF-IDF scores for this genre\n",
    "        avg_tfidf = genre_tfidf.mean(axis=0).A1\n",
    "        \n",
    "        # Get top features\n",
    "        top_indices = avg_tfidf.argsort()[-n_top_features:][::-1]\n",
    "        top_features = [(feature_names[i], avg_tfidf[i]) for i in top_indices]\n",
    "        genre_features[genre] = top_features\n",
    "    \n",
    "    return genre_features\n",
    "\n",
    "# Prepare the data\n",
    "X = df['Text']  # All text data\n",
    "y = df['Genre']  # All genre labels\n",
    "\n",
    "# Create train/test split\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=12,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        input='content',\n",
    "        lowercase=True,\n",
    "        stop_words=stop_words,\n",
    "        tokenizer=WordLemmaTokenizer()\n",
    "    )),\n",
    "\n",
    "    ('clf', ComplementNB())\n",
    "])\n",
    "\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [2, 3, 5],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidf__max_features': [None, 5000, 10000],\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'clf__alpha': [0.1, 0.5, 1.0, 2.0],\n",
    "    'clf__norm': [True, False]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=le.classes_\n",
    "))\n",
    "\n",
    "# If you want to analyze genre-specific features using the best vectorizer\n",
    "best_vectorizer = grid_search.best_estimator_.named_steps['tfidf']\n",
    "X_transformed = best_vectorizer.transform(X)\n",
    "genre_features = analyze_genre_specific_features(df, best_vectorizer)\n",
    "\n",
    "# Print top features for each genre\n",
    "print(\"\\nTop features by genre:\")\n",
    "for genre, features in genre_features.items():\n",
    "    print(f\"\\n{genre}:\")\n",
    "    for feature, score in features:\n",
    "        print(f\"  - {feature}: {score:.4f}\")\n",
    "\n",
    "# Optional: Save best model\n",
    "import joblib\n",
    "joblib.dump(grid_search.best_estimator_, 'best_genre_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68d54e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CUSTOM PIPELINE TO INCLUDE RUNNING TFIDF ON EAC INDIVIDUAL GENRE - FEATURE ENGINEERING\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GenreSpecificVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer that fits separate TF-IDF vectorizers for each genre\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorizer_params=None):\n",
    "        self.vectorizer_params = vectorizer_params or {}\n",
    "        self.vectorizers = {}\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Create a vectorizer for each genre\n",
    "        unique_genres = np.unique(y)\n",
    "        all_features = set()\n",
    "        \n",
    "        # First pass: fit vectorizers and collect all feature names\n",
    "        for genre in unique_genres:\n",
    "            genre_mask = y == genre\n",
    "            genre_texts = X[genre_mask]\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(**self.vectorizer_params)\n",
    "            vectorizer.fit(genre_texts)\n",
    "            self.vectorizers[genre] = vectorizer\n",
    "            all_features.update(vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # Convert to sorted list for consistent feature ordering\n",
    "        self.feature_names = sorted(list(all_features))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if y is None:\n",
    "            # During prediction, transform with all vectorizers\n",
    "            features_matrix = np.zeros((len(X), len(self.feature_names)))\n",
    "            for genre, vectorizer in self.vectorizers.items():\n",
    "                # Transform the texts using this genre's vectorizer\n",
    "                genre_features = vectorizer.get_feature_names_out()\n",
    "                genre_tfidf = vectorizer.transform(X)\n",
    "                \n",
    "                # Map the features to the correct positions\n",
    "                for i, feature in enumerate(genre_features):\n",
    "                    if feature in self.feature_names:\n",
    "                        feature_idx = self.feature_names.index(feature)\n",
    "                        features_matrix[:, feature_idx] += genre_tfidf[:, i].toarray().flatten()\n",
    "            \n",
    "            return features_matrix\n",
    "        \n",
    "        else:\n",
    "            # During training, transform each text with its genre's vectorizer\n",
    "            features_matrix = np.zeros((len(X), len(self.feature_names)))\n",
    "            for genre in self.vectorizers.keys():\n",
    "                genre_mask = y == genre\n",
    "                if not any(genre_mask):\n",
    "                    continue\n",
    "                    \n",
    "                genre_texts = X[genre_mask]\n",
    "                vectorizer = self.vectorizers[genre]\n",
    "                genre_features = vectorizer.get_feature_names_out()\n",
    "                genre_tfidf = vectorizer.transform(genre_texts)\n",
    "                \n",
    "                # Map the features to the correct positions\n",
    "                for i, feature in enumerate(genre_features):\n",
    "                    if feature in self.feature_names:\n",
    "                        feature_idx = self.feature_names.index(feature)\n",
    "                        features_matrix[genre_mask, feature_idx] = genre_tfidf[:, i].toarray().flatten()\n",
    "            \n",
    "            return features_matrix\n",
    "\n",
    "# Prepare the data\n",
    "X = df['Text'].values  # Convert to numpy array\n",
    "y = df['Genre'].values\n",
    "\n",
    "# Create train/test split\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=12,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Create parameter grid for vectorizer\n",
    "vectorizer_param_grid = {\n",
    "    'vectorizer_params': [\n",
    "        {\n",
    "            'analyzer': 'word',\n",
    "            'input': 'content',\n",
    "            'lowercase': True,\n",
    "            'stop_words': stop_words,\n",
    "            'min_df': min_df,\n",
    "            'ngram_range': ngram_range,\n",
    "            'tokenizer': WordLemmaTokenizer()\n",
    "        }\n",
    "        for min_df in [2, 3, 5]\n",
    "        for ngram_range in [(1, 1), (1, 2), (1, 3)]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create parameter grid for classifier\n",
    "clf_param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0],\n",
    "    'norm': [True, False]\n",
    "}\n",
    "\n",
    "# Function to perform grid search\n",
    "def perform_grid_search(X_train, y_train, X_test, y_test):\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_vectorizer = None\n",
    "    best_clf = None\n",
    "    \n",
    "    for vectorizer_params in vectorizer_param_grid['vectorizer_params']:\n",
    "        # Create and fit genre-specific vectorizer\n",
    "        vectorizer = GenreSpecificVectorizer(vectorizer_params)\n",
    "        X_train_transformed = vectorizer.fit_transform(X_train, y_train)\n",
    "        X_test_transformed = vectorizer.transform(X_test)\n",
    "        \n",
    "        # Grid search for classifier\n",
    "        clf_grid = GridSearchCV(\n",
    "            ComplementNB(),\n",
    "            clf_param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1_weighted',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        clf_grid.fit(X_train_transformed, y_train)\n",
    "        \n",
    "        # Check if this combination gives better results\n",
    "        score = clf_grid.score(X_test_transformed, y_test)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {\n",
    "                'vectorizer': vectorizer_params,\n",
    "                'classifier': clf_grid.best_params_\n",
    "            }\n",
    "            best_vectorizer = vectorizer\n",
    "            best_clf = clf_grid.best_estimator_\n",
    "    \n",
    "    return best_vectorizer, best_clf, best_params, best_score\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search...\")\n",
    "best_vectorizer, best_clf, best_params, best_score = perform_grid_search(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(\"Vectorizer parameters:\", best_params['vectorizer'])\n",
    "print(\"Classifier parameters:\", best_params['classifier'])\n",
    "print(\"\\nBest score:\", best_score)\n",
    "\n",
    "# Make predictions with best model\n",
    "X_test_transformed = best_vectorizer.transform(X_test)\n",
    "y_pred = best_clf.predict(X_test_transformed)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=le.classes_\n",
    "))\n",
    "\n",
    "# Optional: Save best model\n",
    "import joblib\n",
    "model_dict = {\n",
    "    'vectorizer': best_vectorizer,\n",
    "    'classifier': best_clf,\n",
    "    'label_encoder': le\n",
    "}\n",
    "joblib.dump(model_dict, 'best_genre_specific_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "53d21950",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_tfidf_df.drop(columns=['Genre'])  # Feature columns (TF-IDF scores)\n",
    "y = combined_tfidf_df['Genre']                 # Target column (genre)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e0ed711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning:\n",
      "\n",
      "\n",
      "50 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py\", line 663, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 981, in check_X_y\n",
      "    check_consistent_length(X, y)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [7486, 508]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py\", line 663, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py\", line 523, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 981, in check_X_y\n",
      "    check_consistent_length(X, y)\n",
      "  File \"/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [7486, 509]\n",
      "\n",
      "\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "\n",
      "/Users/simoncrouch/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning:\n",
      "\n",
      "Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7486, 636]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [183]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     32\u001b[0m     pipeline,\n\u001b[1;32m     33\u001b[0m     param_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Fit the grid search\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Print best parameters and score\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:926\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py:394\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    393\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:663\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    666\u001b[0m     labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:523\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[0;34m(self, X, y, reset)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    964\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    965\u001b[0m     X,\n\u001b[1;32m    966\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    977\u001b[0m )\n\u001b[1;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m--> 981\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    335\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7486, 636]"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Prepare the data\n",
    "X = df['Text']  # All text data\n",
    "y = df['Genre']  # All genre labels\n",
    "\n",
    "# Create train/test split\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=12,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "\n",
    "# Create a pipeline that combines vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        input='content',\n",
    "        lowercase=True,\n",
    "        stop_words=stop_words,\n",
    "        tokenizer=WordLemmaTokenizer()\n",
    "    )),\n",
    "    ('clf', ComplementNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for both vectorizer and classifier\n",
    "param_grid = {\n",
    "    'tfidf__min_df': [2, 3, 5],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidf__max_features': [None, 5000, 10000],\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'clf__alpha': [0.1, 0.5, 1.0, 2.0],\n",
    "    'clf__norm': [True, False]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Make predictions with best model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=le.classes_\n",
    "))\n",
    "\n",
    "# Optional: Save best model\n",
    "#import joblib\n",
    "#joblib.dump(grid_search.best_estimator_, 'best_genre_classifier.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
